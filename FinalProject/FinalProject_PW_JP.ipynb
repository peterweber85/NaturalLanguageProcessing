{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project NLP: Binary Sentiment Analysis\n",
    "\n",
    "Members: Jonatan Pi√±ol and Peter Weber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the attached final_project_lib.py file, there is the class that implements all the logic behind the analysis presented in this notebook.\n",
    "\n",
    "In terms of data preprocessing we implement the following steps.\n",
    "- Reading of the documents, and saving the reviews and the corresponding sentiments separately.\n",
    "- Cleaning the reviews, and converting the sentiments to boolean 0/1.\n",
    "The cleaning step removes punctuation, not alphanumeric tokens, stopwords, and tokens of length one.\n",
    "\n",
    "In terms of data preparation for the algorithm, we use three methods.\n",
    "1. ngram tf-idf, where we use uni-gram, bi-gram, and tri-gram features. We limit the features to the 50000 most occuring ones.\n",
    "2. ngram counts, where we use uni-gram, bi-gram, and tri-gram features. We limit the features to the 50000 most occuring ones\n",
    "3. gensim document2vector embeddings, where each word is characterized by a 2000 elements vector. We build a feature matrix by characterizing every review by its min, mean, and max for every vector element of the embedding, so that we obtain a matrix with 6000 features.\n",
    "\n",
    "For the predictions we use a random forest with 1000 estimators, and 4-fold cross validation to validate the model.\n",
    "We first evaluate every model separately, and then perform a majority vote between the three models to get the final predictions.\n",
    "\n",
    "In a next step (which is not implemented), one would ideally feed the embeddings word by word into a Convolutional Neural Network or an LSTM, to take advantage of the embedding representation of the reviews and the word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment Analysis Class in lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import final_project_lib as lib\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class instantiation and data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded!\n",
      "Documents cleaned!\n"
     ]
    }
   ],
   "source": [
    "instance = lib.SentimentAnalysis()\n",
    "instance.read_documents()\n",
    "instance.clean_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute cross validation score for different algorithms\n",
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and tri-gram tfidf 81.0 %\n"
     ]
    }
   ],
   "source": [
    "X_tfidf, y_tfidf = instance.create_tfidf_matrix(max_feat = 50000, n = 3)\n",
    "score, y_hat_tfidf = instance.cross_validate_random_forest(X_tfidf, y_tfidf)\n",
    "print \"Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and tri-gram tfidf\", \\\n",
    "        round(np.mean(score)*100), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tri-gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and tri-gram counts 84.0 %\n"
     ]
    }
   ],
   "source": [
    "X_ngram, y_ngram = instance.create_ngram_matrix(max_feat = 50000, n = 3)\n",
    "score, y_hat_ngram = instance.cross_validate_random_forest(X_ngram, y_ngram)\n",
    "print \"Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and tri-gram counts\", \\\n",
    "        round(np.mean(score)*100), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document 2 Vector embeddings\n",
    "https://stackoverflow.com/questions/45170589/how-word2vec-deal-with-the-end-of-a-sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = instance.train_doc2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and doc2vec embeddings 75.0 %\n"
     ]
    }
   ],
   "source": [
    "X_d2v, y_d2v = instance.create_embedding_matrix(model)\n",
    "score, y_hat_d2v = instance.cross_validate_random_forest(X_d2v, y_d2v)\n",
    "print \"Cross validation (cv) accuracy using 4-fold cv, a random forest classifier and doc2vec embeddings\", \\\n",
    "        round(np.mean(score)*100), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do majority vote between the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = instance.vote_majority(y_hat_tfidf, y_hat_ngram, y_hat_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify that all target vectors are equal, n_gram == tfidf: True ! tfidf == doc2vec: True !\n",
      "So that any of the three can be chosen for comparison with predictions!\n",
      "\n",
      "Majority vote accuracy: 83.0 %\n"
     ]
    }
   ],
   "source": [
    "print \"Verify that all target vectors are equal, n_gram == tfidf:\", y_ngram == y_tfidf, \\\n",
    "      \"! tfidf == doc2vec:\", y_tfidf == y_d2v, \"!\\n\", \"So that any of the three can be chosen for comparison with predictions!\"\n",
    "accuracy = accuracy_score(y_hat, y_ngram)\n",
    "print \"\\nMajority vote accuracy:\", round(accuracy*100), \"%\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
