{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP course (2017 - 2018)\n",
    "\n",
    "## Homework 1 : Tokenization and Corpus Statistics\n",
    "\n",
    "Peter Weber and Jonatan Piñol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "1) Load and tokenize the treebank corpus from NLTK using regexp_tokenizer\n",
    "- obtain the corpus using get_corpus_t1()\n",
    "- obtain the gold standard using get_gold_tokens()\n",
    "- extend the existing regexp grammar to improve its coverage\n",
    "- modify the corpus prior to the tokenization (if needed)\n",
    "- tokenize the corpus with regexp_tokenize()\n",
    "- evaluate the tokenization using evaluate_t1()\n",
    "- improve the regexp grammar until satisfied with the result\n",
    "\n",
    "2) Print basic statistics for the corpus (after the tokenization)\n",
    "- The number of tokens in the corpus\n",
    "- The number of types in the corpus (case insensitive!)\n",
    "- The number of hapaxes - tokens that appear in the corpus only once (case insensitive!) \n",
    "- The most frequent types with length >=5 \n",
    "- The average token length\n",
    "- The most frequent token length in the corpus\n",
    "- The number of bi-, tri-, and five-grams in the corpus (you need to write your own function for extracting five-grams);\n",
    "- The most frequent bi- and tri-grams that do NOT contain punctuation (for the task, assume punctuation to be , . ! ? )\n",
    "- The most frequent five-grams\n",
    "- The percentage of bi-,tri-, and five-grams that appear only once\n",
    "- The 10 most frequent collocates of \"man\" and \"woman\" in the corpus, within a window of 4\n",
    "- The 10 most frequent collocates of \"man\" and \"woman\", with a frequency of 5 or more, according to the PPMI score (within a window of 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Import regular expressions\n",
    "import re\n",
    "\n",
    "# Import corpora\n",
    "from nltk.corpus import treebank_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Functions given in the task\n",
    "## You should not change anything here\n",
    "def get_corpus_t1(nr_files=199):\n",
    "    \"\"\"Returns the raw corpus as a long string.\n",
    "    'nr_files' says how much of the corpus is returned;\n",
    "    default is 199, which is the whole corpus.\n",
    "    \"\"\"\n",
    "    fileids = nltk.corpus.treebank_raw.fileids()[:nr_files]\n",
    "    corpus_text = nltk.corpus.treebank_raw.raw(fileids)\n",
    "    # Get rid of the \".START\" text in the beginning\tof each file:\n",
    "    corpus_text = corpus_text.replace(\".START\", \"\")\n",
    "    return corpus_text\n",
    "\n",
    "def fix_gold_tokens(tokens):\n",
    "    \"\"\"Replace tokens so that they are similar to the raw corpus text.\"\"\"\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\",'\"').replace(r\"\\/\", \"/\") for token in tokens]\n",
    "\n",
    "def get_gold_tokens(nr_files=199):\n",
    "    \"\"\"Returns the gold corpus as a list of strings.\n",
    "    'nr_files' says how much of the corpus is returned;\n",
    "    default is 199, which is the whole corpus.\n",
    "    \"\"\"\n",
    "    fileids = nltk.corpus.treebank_chunk.fileids()[:nr_files]\n",
    "    gold_tokens = nltk.corpus.treebank_chunk.words(fileids)\n",
    "    return fix_gold_tokens(gold_tokens)\n",
    "\n",
    "def evaluate_t1(test_tokens, gold_tokens):\n",
    "    \"\"\"Finds the chunks where test_tokens differs from gold_tokens.\n",
    "    Prints the errors and calculates similarity measures.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    matcher = difflib.SequenceMatcher()\n",
    "    matcher.set_seqs(test_tokens, gold_tokens)\n",
    "    error_chunks = true_positives = false_positives = false_negatives = 0\n",
    "    print(\" Token%30s | %-30sToken\" % (\"Error\", \"Correct\"))\n",
    "    print(\"-\" * 38 + \"+\" + \"-\" * 38)\n",
    "    for difftype, test_from, test_to, gold_from, gold_to in matcher.get_opcodes():\n",
    "        if difftype == \"equal\":\n",
    "            true_positives += test_to - test_from\n",
    "        else:\n",
    "            false_positives += test_to - test_from\n",
    "            false_negatives += gold_to - gold_from\n",
    "            error_chunks += 1\n",
    "            test_chunk = \" \".join(test_tokens[test_from:test_to])\n",
    "            gold_chunk = \" \".join(gold_tokens[gold_from:gold_to])\n",
    "            print(\"%6d%30s | %-30s%d\" % (test_from,test_chunk, gold_chunk, gold_from))\n",
    "    precision = 1.0 * true_positives / (true_positives + false_positives)\n",
    "    recall = 1.0 * true_positives / (true_positives+ false_negatives)\n",
    "    fscore = 2.0 * precision * recall / (precision+ recall)\n",
    "    print()\n",
    "    print(\"Test size: %5d tokens\" % len(test_tokens))\n",
    "    print(\"Gold size: %5d tokens\" % len(gold_tokens))\n",
    "    print(\"Nr errors: %5d chunks\" % error_chunks)\n",
    "    print(\"Precision: %5.2f %%\" % (100 * precision))\n",
    "    print(\"Recall: %5.2f %%\" % (100 * recall))\n",
    "    print(\"F-score: %5.2f %%\" % (100 * fscore))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HOMEWORK 1. PART 1.\n",
    "# Dummy function\n",
    "# Feel free to make it more verbose and include prints/status updates\n",
    "def hw1_part1():\n",
    "    # Get the corpus\n",
    "    print(\"\\n------------------------- FIRST PART ---------------------------------\")\n",
    "    corpus = get_corpus_t1()\n",
    "    # Modification on the corpus in order to tokenize \"n't\"\n",
    "    corpus = re.sub('n\\'t', ' n\\'t', corpus)\n",
    "    \n",
    "    # Get the gold standard\n",
    "    gold_tokens = get_gold_tokens()\n",
    "\n",
    "    # Initial regular expression grammar\n",
    "    # You need to modify it so that you can improve the performance of the tokenizer\n",
    "    re_grammar = r'''(?x)  # set flag to allow verbose regexps\n",
    "     \\'[a-z][a-z]?                 \n",
    "     | (?:n\\'t)                           # n't\n",
    "     | Corp\\. | Calif\\. | Sept\\. | Conn\\. # usual abbreviations\n",
    "     |[A-Z][a-z]{1,2}\\.                  # more abreviations\n",
    "     | [A-Z]*[a-z]+\\/[A-Z]*[a-z]+     \n",
    "     | \\d+(?:\\,\\d+)*\\-\\w+\\-?\\w*         # 19-years-old\n",
    "     | [A-Z][A-Z]?\\&[A-Z]\n",
    "     | [\\$\\%]                           # dollar symbol and percentage symbol\n",
    "     | \\'?[0-9]+s                       # 1950s\n",
    "     | \\d+\\:\\d{2}\\s[ap]\\.m              # 12:30 p.m\n",
    "     | \\d+(?:[\\.\\,\\/\\-]\\d+)?(?:[\\,\\-\\/\\-]\\d+)?           # numbers, e.g. 12.40, 82,2, 2/7,9-0, 122,222,222\n",
    "     | (?:[A-Z]\\.)+\\.?                  # abbreviations, e.g. U.S.A.\n",
    "     | (?:[a-z]\\.)+\\.?\n",
    "     | \\w+(?:-\\w+)*                     # words with optional internal hyphens\n",
    "     | \\.\\.\\.                           # ellipsis\n",
    "     | \\-\\-\n",
    "     | [][.,;\"'?!():-_`#&{}]            # these are separate tokens; includes ], [\n",
    "\n",
    "    ''' \n",
    "\n",
    "    # Modify the corpus prior to tokenization here, if necessary\n",
    "    \n",
    "    # Tokenize the corpus\n",
    "    test_tokens = regexp_tokenize(corpus, re_grammar)\n",
    "    \n",
    "    # Evaluate the results\n",
    "    evaluate_t1(test_tokens,gold_tokens)\n",
    "    \n",
    "    return(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HOMEWORK 1. PART 2.\n",
    "# Dummy function\n",
    "# Feel free to make it more verbose and include prints/status updates\n",
    "def hw1_part2(tokens):\n",
    "    \n",
    "    print(\"\\n-------------------------- SECOND PART -----------------------------\")\n",
    "    print(\"\\nThis function prints the corpus statistics for the SECOND PART of the HOMEWORK 1\")\n",
    "    print(\"\\n -------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # all tokens lowercase, for case insensitiveness. We asume that for n-grams statistics should also be case insensitive\n",
    "    words = [w.lower() for w in tokens] \n",
    "\n",
    "    # number of tokens in the corpus\n",
    "    n_tokens = len(words)\n",
    "    n_tokens\n",
    "    print(\"\\nThe number of tokens is : \",n_tokens)\n",
    "\n",
    "    # number of types\n",
    "    types = set(words)\n",
    "    n_types = len(types) # case insensitve! \n",
    "    n_types\n",
    "    print(\"\\nThe number of types is : \",n_types)\n",
    "\n",
    "    # number of hapaxes\n",
    "    fdist = FreqDist(words)\n",
    "    n_hapaxes = len(fdist.hapaxes()) # case insensitve! \n",
    "    n_hapaxes\n",
    "    print(\"\\nThe number of hapaxes is : \",n_hapaxes)\n",
    "\n",
    "    # most frequent types with length >= 5\n",
    "    long_tokens = [w for w in words if len(w)>=5] # filter tokens with length >=5\n",
    "    fdist2 = FreqDist(long_tokens)  \n",
    "    frequent_long_types = fdist2.most_common(10) # most common types \n",
    "    frequent_long_types\n",
    "    print(\"\\nThe most frequent types with length >= 5 are (case insensitive!!): \",frequent_long_types)\n",
    "\n",
    "    # average token length\n",
    "    average_lenght =  sum(len(w) for w in words)/n_tokens\n",
    "    average_lenght\n",
    "    print(\"\\nThe average token length is: \",average_lenght)\n",
    "\n",
    "    # The most frequent token length in text\n",
    "    token_lengths = (len(w) for w in words)\n",
    "    fdist3 = FreqDist(token_lengths)\n",
    "    most_frequent_length = fdist3.most_common(1)[0][0]\n",
    "    most_frequent_length\n",
    "    print(\"\\nThe most frequent token length in text: \",most_frequent_length)\n",
    "\n",
    "    # the number of bi-grams, tri-grams, five-grams \n",
    "    # bi-grams\n",
    "    bigr_list = list(bigrams(words))\n",
    "    n_bigr = len(bigr_list)\n",
    "    print(\"\\nThe number of bigrams is : \",n_bigr)\n",
    "    # tri-grams\n",
    "    trigr_list = list(trigrams(words))\n",
    "    n_trigr = len(trigr_list)\n",
    "    print(\"\\nThe number of trigrams is : \",n_trigr)\n",
    "    # five-grams\n",
    "    from nltk import ngrams\n",
    "    fivegrams = ngrams(words, 5)\n",
    "    fivegr_list = list(grams for grams in fivegrams)\n",
    "    n_fivegr = len(fivegr_list)\n",
    "    print(\"\\nThe number of fivegrams is : \",n_fivegr)\n",
    "\n",
    "    # The most frequent bi- and tri-grams that do NOT contain punctuation (for the task, assume punctuation to be , . ! ? )\n",
    "    # bi-gram\n",
    "    words_no_punctuation = list(w for w in words if re.search('.*[^,\\.\\!\\?].*',w))\n",
    "    bigr_list2 = list(bigrams(words_no_punctuation))\n",
    "    fdist4 = FreqDist(bigr_list2)\n",
    "    print(\"\\nMost frequent bigram\", fdist4.most_common(10))\n",
    "    #tri-gram\n",
    "    trigr_list2 = list(trigrams(words_no_punctuation))\n",
    "    fdist5 = FreqDist(trigr_list2)\n",
    "    print(\"\\nMost frequent trigram\",fdist5.most_common(10))\n",
    "\n",
    "    # The most frequent five-gram (as the opposite is not stated, in this case 5-grams could contain punctuation):\n",
    "    fdist6 = FreqDist(fivegr_list)\n",
    "    print(\"\\nMost frequent fivegram\",fdist6.most_common(10))\n",
    "\n",
    "    # The percentage of bi-,tri-, and five-grams that appear only once, that MAY contain some punctuation\n",
    "    fdist7 = FreqDist(bigr_list)\n",
    "    bigr_onetime = len(fdist7.hapaxes())\n",
    "    bigr_onetime_perc = (bigr_onetime/len(bigr_list))*100\n",
    "    print(\"\\nPercentage of bigrams that appear once \",bigr_onetime_perc,\"%\" )\n",
    "\n",
    "    fdist8 = FreqDist(trigr_list)\n",
    "    trigr_onetime = len(fdist8.hapaxes())\n",
    "    trigr_onetime_perc = (trigr_onetime/len(trigr_list))*100\n",
    "    print(\"\\nPercentage of trigrams that appear once \",trigr_onetime_perc,\"%\" )\n",
    "\n",
    "    fdist9 = FreqDist(fivegr_list)\n",
    "    fivegr_onetime = len(fdist9.hapaxes())\n",
    "    fivegr_onetime_perc = (fivegr_onetime/len(fivegr_list))*100\n",
    "    print(\"\\nPercentage of fivegrams that appear once \",fivegr_onetime_perc,\"%\" )\n",
    "\n",
    "    # The 10 most frequent collocates of \"man\" and \"woman\" in the corpus, within a window of 4\n",
    "    cooc_ = BigramCollocationFinder.from_words(words, window_size=5)\n",
    "\n",
    "    cooc_man_list = []\n",
    "    cooc_woman_list = []\n",
    "    for pair,freq in cooc_.ngram_fd.items():\n",
    "        if 'man' in pair:\n",
    "            cooc_man_list.append([pair,freq])\n",
    "        elif 'woman' in pair:\n",
    "            cooc_woman_list.append([pair,freq])\n",
    "    cooc_man_list = sorted(cooc_man_list, key=lambda x: x[1],reverse=True)\n",
    "    cooc_woman_list = sorted(cooc_woman_list, key=lambda x: x[1],reverse=True)\n",
    "    print(\"\\nCo-occurrence of 'man' in a window of 4 : \",cooc_man_list[:10])\n",
    "    print(\"\\nCo-occurrence of 'woman' in a window of 4 : \",cooc_woman_list[:10])\n",
    "\n",
    "    # The 10 most frequent collocates of \"man\" and \"woman\", with a frequency of 5 or more, according to the PPMI score\n",
    "    # (within a window of 4)\n",
    "    # For this part we assume that what is asked is to output the top 10 \"man\"/\"woman\" collocates of highest PPMI score \n",
    "    # Load the pre-built association measures\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    cooc_.score_ngrams(bigram_measures.pmi)\n",
    "    cooc_man_list = []\n",
    "    cooc_woman_list = []\n",
    "    for pair,freq in cooc_.score_ngrams(bigram_measures.pmi):\n",
    "        if 'man' in pair:\n",
    "            cooc_man_list.append([pair,freq])\n",
    "        if 'woman' in pair:\n",
    "            cooc_woman_list.append([pair,freq])\n",
    "    cooc_man_list_top = list(bigram for bigram in cooc_man_list if bigram[1]>=5)\n",
    "    cooc_woman_list_top = list(bigram for bigram in cooc_woman_list if bigram[1]>=5)\n",
    "    print(\"\\nThe 10 most frequent collocates of 'man', with a frequency of 5 or more, according to the PPMI score :\",cooc_man_list_top[:10])\n",
    "    print(\"\\nThe 10 most frequent collocates of 'woman', with a frequency of 5 or more, according to the PPMI score :\",cooc_woman_list_top[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------- FIRST PART ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonatanpinol/anaconda3/envs/NLTK/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token                         Error | Correct                       Token\n",
      "--------------------------------------+--------------------------------------\n",
      "   444                        Mass . | Mass.                         444\n",
      "   818                      Donoghue |                               817\n",
      "  1300                         S. p. | S.p .                         1298\n",
      "  1499                         Corp. | Corp .                        1497\n",
      "  2700                        Mass . | Mass.                         2699\n",
      "  2757                         Conn. | Conn .                        2755\n",
      "  2917                         . . . | ...                           2916\n",
      "  3330                         . . . | ...                           3327\n",
      "  4502                          Ltd. | Ltd .                         4497\n",
      "  4741                        Colo . | Colo.                         4737\n",
      "  4790                      Messrs . | Messrs.                       4785\n",
      "  6852                          US $ | US$                           6846\n",
      "  6872                           C $ | C$                            6865\n",
      "  7140                           Co. | Co .                          7132\n",
      "  7253                          U.S. | U.S .                         7246\n",
      "  7431                      S. p. A. | S.p.A.                        7425\n",
      "  8401                          U.S. | U.S .                         8393\n",
      "  8513                          U.S. | U.S .                         8506\n",
      "  8704                          Act. | Act .                         8698\n",
      "  9185                         Corp. | Corp .                        9180\n",
      "  9600                         Conn. | Conn .                        9596\n",
      " 11280                          U.S. | U.S .                         11277\n",
      " 11839                         . . . | ...                           11837\n",
      " 11917                          US $ | US$                           11913\n",
      " 11954                           C $ | C$                            11949\n",
      " 13108                          Ltd. | Ltd .                         13102\n",
      " 14206                       C 'mo n | C'mon                         14201\n",
      " 14654                         . . . | ...                           14647\n",
      " 14790                    O ' Connor | O'Connor                      14781\n",
      " 14817                    O ' Connor | O'Connor                      14806\n",
      " 14985                         Corp. | Corp .                        14972\n",
      " 15204                 U.S. Japanese | U.S.-Japanese                 15192\n",
      " 16035                         . . . | ...                           16022\n",
      " 16051                         . . . | ...                           16036\n",
      " 16430                          Act. | Act .                         16413\n",
      " 16641        Macmillan/Mc Graw-Hill | Macmillan/McGraw-Hill         16625\n",
      " 17534                         11 th | 11th                          17517\n",
      " 18407                          Mrs. | Mrs .                         18389\n",
      " 18977        Macmillan/Mc Graw-Hill | Macmillan/McGraw-Hill         18960\n",
      " 19023        Macmillan/Mc Graw-Hill | Macmillan/McGraw-Hill         19005\n",
      " 19041                         Corp. | Corp .                        19022\n",
      " 19132                        Mich . | Mich.                         19114\n",
      " 19263             Macmillan/Mc Graw | Macmillan/McGraw              19244\n",
      " 19345             Macmillan/Mc Graw | Macmillan/McGraw              19325\n",
      " 19427Macmillan/Mc Graw , the Macmillan/Mc Graw | Macmillan/McGraw , the Macmillan/McGraw19406\n",
      " 19707                      Messrs . | Messrs.                       19684\n",
      " 19825  Macmillan/Mc Graw . Messrs . | Macmillan/McGraw . Messrs.    19801\n",
      " 21787                        cannot | can not                       21761\n",
      " 22127                          May. | May .                         22102\n",
      " 22422                        Calif. | Calif .                       22398\n",
      " 22562                         . . . | ...                           22539\n",
      " 22646                   CREATOR ' S | CREATOR'S                     22621\n",
      " 23505                          Inc. | Inc .                         23478\n",
      " 24056                    U.S. Japan | U.S.-Japan                    24030\n",
      " 24500                          U.S. | U.S .                         24473\n",
      " 24593                     a-Average | a - Average                   24567\n",
      " 24616                     b-Current | b - Current                   24592\n",
      " 24987                           Co. | Co .                          24965\n",
      " 25048                           Co. | Co .                          25027\n",
      " 25053                          N.J. | N.J .                         25033\n",
      " 26172                           Co. | Co .                          26153\n",
      " 27165                         . . . | ...                           27147\n",
      " 28252                          U.S. | U.S .                         28232\n",
      " 28342                           Mo. | Mo .                          28323\n",
      " 28670                          Ill. | Ill .                         28652\n",
      " 29120                    N.J. based | N.J.-based                    29103\n",
      " 29253                       1/10 th | 1/10th                        29235\n",
      " 29806                          ' 82 | '82                           29787\n",
      " 30004                         10 th | 10th                          29984\n",
      " 30802                          ' 86 | '86                           30781\n",
      " 31120                           Co. | Co .                          31098\n",
      " 31794                         . . . | ...                           31773\n",
      " 32826                           Co. | Co .                          32803\n",
      " 33519                          U.S. | U.S .                         33497\n",
      " 33734                          U.S. | U.S .                         33713\n",
      " 33751                          U.S. | U.S .                         33731\n",
      " 33896                           Jr. | Jr .                          33877\n",
      " 33920                           Co. | Co .                          33902\n",
      " 34452                        Mass . | Mass.                         34435\n",
      " 34721                          Inc. | Inc .                         34703\n",
      " 34725                        Mass . | Mass.                         34708\n",
      " 34783                          Inc. | Inc .                         34765\n",
      " 35016                         . . . | ...                           34999\n",
      " 35911                        Tenn . | Tenn.                         35892\n",
      " 36481                          N.C. | N.C .                         36461\n",
      " 36824                        Ariz . | Ariz.                         36805\n",
      " 37380                      1:30 p.m | 1:30 p.m                      37360\n",
      " 37397                      3:15 p.m | 3:15 p.m                      37378\n",
      " 37973                        Mass . | Mass.                         37955\n",
      " 38129                        Mich . | Mich.                         38110\n",
      " 39972                           Co. | Co .                          39952\n",
      " 40693                           Co. | Co .                          40674\n",
      " 41063                          vs . | vs.                           41045\n",
      " 41384                        Calif. | Calif .                       41365\n",
      " 41738                   Sino-U . S. | Sino-U.S.                     41720\n",
      " 42701                         . . . | ...                           42681\n",
      " 43132                          Ala. | Ala .                         43110\n",
      " 43504                        Mich . | Mich.                         43483\n",
      " 43519                        Mich . | Mich.                         43497\n",
      " 43677                        Wash . | Wash.                         43654\n",
      " 43689                          N.J. | N.J .                         43665\n",
      " 43891                    62 % owned | 62%-owned                     43868\n",
      " 44048                        Colo . | Colo.                         44023\n",
      " 44110                        Miss . | Miss.                         44084\n",
      " 44305                        Minn . | Minn.                         44278\n",
      " 44582                        Miss . | Miss.                         44554\n",
      " 44609                        Colo . | Colo.                         44580\n",
      " 44686                        Colo . | Colo.                         44656\n",
      " 44758                        Colo . | Colo.                         44727\n",
      " 45443                         . . . | ...                           45411\n",
      " 45882               90-cent-an hour | 90-cent-an-hour               45848\n",
      " 46123                          N.J. | N.J .                         46088\n",
      " 46558                         Corp. | Corp .                        46524\n",
      " 47944                           Pa. | Pa .                          47911\n",
      " 48013            Philippines-backed | Philippines - backed          47981\n",
      " 48900                           ' S | 'S                            48870\n",
      " 50048                        Prof . | Prof.                         50017\n",
      " 50199                          N.J. | N.J .                         50167\n",
      " 50332                          N.Y. | N.Y .                         50301\n",
      " 50653                          N.C. | N.C .                         50623\n",
      " 50670                          US $ | US$                           50641\n",
      " 50973                          Ark. | Ark .                         50943\n",
      " 51673                          U.S. | U.S .                         51644\n",
      " 52256                     O ' Brien | O'Brien                       52228\n",
      " 52400                     O ' Brien | O'Brien                       52370\n",
      " 52568                         . . . | ...                           52536\n",
      " 53819                        U.S.A. | U.S.A .                       53785\n",
      " 53821                          Inc. | Inc .                         53788\n",
      " 53998                        C.D. s | C.D.s                         53966\n",
      " 54176                          7 16 | 7/16                          54143\n",
      " 54336                          Inc. | Inc .                         54302\n",
      " 54386                          Inc. | Inc .                         54353\n",
      " 54593                          U.S. | U.S .                         54561\n",
      " 54777                   Rey Fawcett | Rey/Fawcett                   54746\n",
      " 55017                         . . . | ...                           54985\n",
      " 55118                      Law. . . | Law ...                       55084\n",
      " 55473                          U.S. | U.S .                         55438\n",
      " 57472                          Sun. | Sun .                         57438\n",
      " 57715                          Inc. | Inc .                         57682\n",
      " 61318                         Corp. | Corp .                        61286\n",
      " 61961                         Corp. | Corp .                        61930\n",
      " 62044                          Inc. | Inc .                         62014\n",
      " 62381                        Prof . | Prof.                         62352\n",
      " 63317                   key-someone | key - someone                 63287\n",
      " 64208                         etc . | etc.                          64180\n",
      " 64570                          Act. | Act .                         64541\n",
      " 64707                        Calif. | Calif .                       64679\n",
      " 65213                          Ray. | Ray .                         65186\n",
      " 65293                          Inc. | Inc .                         65267\n",
      " 65493                  O ' Loughlin | O'Loughlin                    65468\n",
      " 65666                          N.J. | N.J .                         65639\n",
      " 65759                        Mass . | Mass.                         65733\n",
      " 65821                         . . . | ...                           65794\n",
      " 67480                          1s t | 1st                           67451\n",
      " 67673                    non-U . S. | non-U.S.                      67643\n",
      " 67772                          Inc. | Inc .                         67740\n",
      " 68214                           Co. | Co .                          68183\n",
      " 68246                          Ltd. | Ltd .                         68216\n",
      " 68342                           Co. | Co .                          68313\n",
      " 68438                           Co. | Co .                          68410\n",
      " 68535                         Corp. | Corp .                        68508\n",
      " 68574                          Ltd. | Ltd .                         68548\n",
      " 69627                    62 % owned | 62%-owned                     69602\n",
      " 69633                           Co. | Co .                          69606\n",
      " 70128                           Co. | Co .                          70102\n",
      " 70650                           Co. | Co .                          70625\n",
      " 70711                          U.S. | U.S .                         70687\n",
      " 71772                          N.V. | N.V .                         71749\n",
      " 73362                        B.A. T | B.A.T                         73340\n",
      " 73397                        B.A. T | B.A.T                         73374\n",
      " 73696                          Inc. | Inc .                         73672\n",
      " 73723                          Inc. | Inc .                         73700\n",
      " 73745                         Corp. | Corp .                        73723\n",
      " 73827                          Inc. | Inc .                         73806\n",
      " 73876                          Inc. | Inc .                         73856\n",
      " 73919                     D ' Amico | D'Amico                       73900\n",
      " 73974                  Calif. based | Calif.-based                  73953\n",
      " 74673                          Inc. | Inc .                         74651\n",
      " 74895                    Őyesterday | yesterday                     74874\n",
      " 74897                             ĺ |                               74876\n",
      " 74918                         Corp. | Corp .                        74896\n",
      " 75007                          14 . | 14.                           74986\n",
      " 75288                       29 year | 29year                        75266\n",
      " 75961                         Corp. | Corp .                        75938\n",
      " 76000                        Mich . | Mich.                         75978\n",
      " 76495                          Ltd. | Ltd .                         76472\n",
      " 76577                          S.A. | S.A .                         76555\n",
      " 77086                          N.C. | N.C .                         77065\n",
      " 77096                     Mo. based | Mo.-based                     77076\n",
      " 78450                          Inc. | Inc .                         78429\n",
      " 80735                          U.S. | U.S .                         80715\n",
      " 81745                      Dunkin ' | Dunkin'                       81726\n",
      " 81768                      Dunkin ' | Dunkin'                       81748\n",
      " 81866                      Dunkin ' | Dunkin'                       81845\n",
      " 81897                      Dunkin ' | Dunkin'                       81875\n",
      " 81964                   Conn. based | Conn.based                    81941\n",
      " 83054                             > |                               83030\n",
      " 83061                           > < |                               83036\n",
      " 83064              In < ternational | International                 83037\n",
      " 84606                          U.S. | U.S .                         84577\n",
      " 85052                  Mich . based | Mich.-based                   85024\n",
      " 85222                          Inc. | Inc .                         85192\n",
      " 85365                   U.S. backed | U.S.-backed                   85336\n",
      " 85748               U.N. supervised | U.N.-supervised               85718\n",
      " 85859                   Sino-U . S. | Sino-U.S.                     85828\n",
      " 86528                      Sept. 30 | Sept.30                       86495\n",
      " 86538                          US $ | US$                           86504\n",
      " 88235                          vs . | vs.                           88200\n",
      " 88272                        Mass . | Mass.                         88236\n",
      " 88778                       G. m.b. | G.m.b .                       88741\n",
      " 89296                           Co. | Co .                          89259\n",
      " 89467                          Inc. | Inc .                         89431\n",
      " 89471                        Ariz . | Ariz.                         89436\n",
      " 90741                          Inc. | Inc .                         90705\n",
      " 91578                          U.S. | U.S .                         91543\n",
      " 92550                     O ' Neill | O'Neill                       92516\n",
      " 93273                          Ltd. | Ltd .                         93237\n",
      " 94021                        Mass . | Mass.                         93986\n",
      "\n",
      "Test size: 94236 tokens\n",
      "Gold size: 94200 tokens\n",
      "Nr errors:   218 chunks\n",
      "Precision: 99.61 %\n",
      "Recall: 99.65 %\n",
      "F-score: 99.63 %\n",
      "\n",
      "\n",
      "-------------------------- SECOND PART -----------------------------\n",
      "\n",
      "This function prints the corpus statistics for the SECOND PART of the HOMEWORK 1\n",
      "\n",
      " -------------------------------------------------------------------------------\n",
      "\n",
      "The number of tokens is :  94236\n",
      "\n",
      "The number of types is :  10940\n",
      "\n",
      "The number of hapaxes is :  5521\n",
      "\n",
      "The most frequent types with length >= 5 are (case insensitive!!):  [('million', 383), ('company', 260), ('which', 225), ('about', 212), ('would', 209), ('market', 187), ('their', 184), ('stock', 172), ('trading', 167), ('president', 161)]\n",
      "\n",
      "The average token length is:  4.4670508086081755\n",
      "\n",
      "The most frequent token length in text:  3\n",
      "\n",
      "The number of bigrams is :  94235\n",
      "\n",
      "The number of trigrams is :  94234\n",
      "\n",
      "The number of fivegrams is :  94232\n",
      "\n",
      "Most frequent bigram [(('of', 'the'), 488), (('in', 'the'), 391), (('for', 'the'), 182), (('to', 'the'), 175), (('on', 'the'), 162), (('the', 'company'), 156), (('the', 'u.s.'), 115), (('said', 'it'), 111), (('\"', 'the'), 109), (('said', 'the'), 109)]\n",
      "\n",
      "Most frequent trigram [(('in', 'new', 'york'), 40), (('the', 'company', \"'s\"), 35), (('the', 'company', 'said'), 31), (('cents', 'a', 'share'), 31), (('in', 'the', 'u.s.'), 29), (('new', 'york', 'stock'), 26), (('york', 'stock', 'exchange'), 26), (('president', 'and', 'chief'), 23), (('the', 'big', 'board'), 21), (('the', 'new', 'york'), 21)]\n",
      "\n",
      "Most frequent fivegram [(('the', 'new', 'york', 'stock', 'exchange'), 15), ((',', 'the', 'company', 'said', '.'), 12), (('president', 'and', 'chief', 'executive', 'officer'), 11), ((',', '\"', 'he', 'said', '.'), 11), (('new', 'york', 'stock', 'exchange', 'composite'), 10), ((',', '\"', 'he', 'says', '.'), 10), (('in', 'new', 'york', 'stock', 'exchange'), 9), (('york', 'stock', 'exchange', 'composite', 'trading'), 9), (('.', 'at', 'the', 'same', 'time'), 8), (('at', 'the', 'same', 'time', ','), 8)]\n",
      "\n",
      "Percentage of bigrams that appear once  45.149891229373374 %\n",
      "\n",
      "Percentage of trigrams that appear once  80.65347963580024 %\n",
      "\n",
      "Percentage of fivegrams that appear once  97.47962475592156 %\n",
      "\n",
      "Co-occurrence of 'man' in a window of 4 :  [[('the', 'man'), 6], [('man', ','), 4], [('man', 'a'), 4], [('a', 'man'), 4], [('and', 'man'), 3], [('.', 'man'), 3], [(',', 'man'), 3], [('man', '.'), 3], [('said', 'man'), 2], [('man', 'for'), 2]]\n",
      "\n",
      "Co-occurrence of 'woman' in a window of 4 :  [[('a', 'woman'), 5], [(',', 'woman'), 4], [('woman', 'an'), 3], [('young', 'woman'), 2], [('woman', ','), 2], [('woman', \"'s\"), 2], [('woman', 'in'), 2], [('that', 'woman'), 2], [('with', 'woman'), 1], [('woman', 'who'), 1]]\n",
      "\n",
      "The 10 most frequent collocates of 'man', with a frequency of 5 or more, according to the PPMI score : [[('beecham', 'man'), 10.716635760426806], [('man', 'ray.'), 10.716635760426806], [('man', 'wick'), 10.716635760426806], [('man', 'z.'), 10.716635760426806], [('portrayal', 'man'), 10.716635760426806], [('tall', 'man'), 10.716635760426806], [('connor', 'man'), 9.716635760426806], [('energetic', 'man'), 9.716635760426806], [('man', 'sweet'), 9.716635760426806], [('sketch', 'man'), 9.716635760426806]]\n",
      "\n",
      "The 10 most frequent collocates of 'woman', with a frequency of 5 or more, according to the PPMI score : [[('close-up', 'woman'), 11.354065681042098], [('disagreeable', 'woman'), 11.354065681042098], [('woman', 'equal-opportunity'), 11.354065681042098], [('woman', 'giraud'), 11.354065681042098], [('woman', 'obtaining'), 11.354065681042098], [('woman', 'shadows'), 11.354065681042098], [('woman', 'solihull'), 11.354065681042098], [('abortionist', 'woman'), 10.354065681042098], [('assist', 'woman'), 10.354065681042098], [('woman', 'observed'), 10.354065681042098]]\n"
     ]
    }
   ],
   "source": [
    "# Main program\n",
    "tokens = hw1_part1()\n",
    "hw1_part2(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
