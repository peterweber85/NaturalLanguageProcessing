{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP course (2017-2018).\n",
    "\n",
    "### Peter Weber & Jonatan Piñol\n",
    "\n",
    "Homework 2.1: Markov Models. Hidden Markov Models and Part of Speech Tagging.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Create a tri-gram model for generating pseudo-Trump sentences \n",
    "- load the corpus, tokenize it and obtain list of trigrams \n",
    "- define a function that obtains the counts of the \"model\" \n",
    "- define a function that generates a pseudo-sentence \n",
    "- when generating a sentence, make sure that your sentence fulfils the following requirements\n",
    "    - it is at least 5 words long\n",
    "    - the last token of the pseudo-sentence is a \".\", \"!\", or \"?\"\n",
    "    - it does not contain any other \".\", \"!\", \"?\" tokens other than the final one\n",
    "- print 5 pseudo-sentences\n",
    "\n",
    "2) Use the built-in n-gram HMM models in nltk to tag a corpus \n",
    "- load the brown corpus\n",
    "- split each category in the corpus to test and train\n",
    "- for each category in the corpus, train on the train set and evaluate on the test set the following taggers:\n",
    "    - default\n",
    "    - affix\n",
    "    - unigram\n",
    "    - bigram\n",
    "    - trigram\n",
    "    \n",
    "    Each tagger should have backoff configured on the previous tagger.\n",
    "    \n",
    "    Print the results in a table.\n",
    "    \n",
    "    \n",
    "- repeat the previous experiment using universal tagset. Print the results in a table.\n",
    "- cross evaluate between different genres (train on one category, evaluate on all the other categories). Print and compare the results\n",
    "- Only for the \"news\" portion of the corpus, compare\n",
    "    - the best berforming tagger (with backoff)\n",
    "    - the naive bayes tagger\n",
    "    \n",
    "    Compare the accuracy as well as the execution time.\n",
    "    \n",
    "    Use both the universal tagset and the full tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import nltk\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import time\n",
    "import time\n",
    "\n",
    "# Import codecs\n",
    "import codecs\n",
    "\n",
    "# Import taggers\n",
    "from nltk import DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk import ClassifierBasedPOSTagger\n",
    "\n",
    "# Import the brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Additional modules\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM IS WONDERFUL IN MANY WAYS , BUT FIVE AT LEAST PEOPLE LOOKING FOR HIM .\n",
      "THE AIR-CONDITIONERS TO COME IN AND THEY STRIP APART TO USE FOR OUR COUNTRY , BUT THEY 'LL SAY , MAN , DID THEY TAKE THAT .\n",
      "Israel will force the issue , but I want to go over – well , how long are we get .\n",
      "`` Well , we had the killing of Jamiel and the decisions that are unbelievable , what he did a big business .\n",
      "I can ’ t even want to leave on such and such a believer and I hope you all very much everybody .\n"
     ]
    }
   ],
   "source": [
    "# Homework 2 part 1\n",
    "\n",
    "def get_markov_stats(trigrams):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        (list) trigrams\n",
    "    \n",
    "    Output: \n",
    "        (dict: key = str, value = list) \n",
    "            key: string of first two words in trigram  \n",
    "            value: list of all possible third words in trigram\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    markov_stats = {}\n",
    "    \n",
    "    for words in trigrams:\n",
    "        words_list = list(words)\n",
    "        word_12 = \" \".join([words_list[0], words_list[1]])\n",
    "        word_3 = words_list[2]\n",
    "        \n",
    "        # Check if there is an entry for the current word\n",
    "        if word_12 in markov_stats.keys():\n",
    "            # If it is, append the second one\n",
    "            markov_stats[word_12].append(word_3)\n",
    "\n",
    "        # If it isn't, create it with the corresponding value\n",
    "        else:\n",
    "            markov_stats[word_12] = [word_3]\n",
    "    return(markov_stats)\n",
    "\n",
    "def generate_sentence(corpus, stats):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        corpus: (list) corpus of words\n",
    "        stats: output of get_markov_stats\n",
    "    Output:\n",
    "        prints sentence according to rules given in assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get first two words of sentence, excluding .!?\n",
    "    first_bigram = list(random.choice(list(nltk.bigrams(corpus[:-1]))))\n",
    "    while \".\" in first_bigram or \"!\" in first_bigram or \"?\" in first_bigram or \",\" in first_bigram or \"’\" in first_bigram or first_bigram[0].islower():\n",
    "        first_bigram = list(random.choice(list(nltk.bigrams(corpus[:-1]))))\n",
    "    \n",
    "    new_speech = first_bigram\n",
    "\n",
    "    # Generate a sentence of minimum length 5\n",
    "    length = 1\n",
    "    while True:\n",
    "        # Get next word from previous two words\n",
    "        next_word = np.random.choice(stats[\" \".join(new_speech[-2:])])\n",
    "        \n",
    "        # Is sentence shorter than 5 words?\n",
    "        if length < 5: \n",
    "            # If it finds punctuation restart\n",
    "            if \".\" in next_word or \"!\" in next_word or \"?\" in next_word:\n",
    "                return(generate_sentence(corpus = corpus, stats = stats))  \n",
    "            # If no punctuation append word to existing sentence\n",
    "            else:\n",
    "                new_speech.append(next_word)\n",
    "                length += 1\n",
    "                \n",
    "        # Is sentence at least 5 words long?\n",
    "        elif length >= 5:\n",
    "            # If includes punctuation return\n",
    "            if \".\" in next_word or \"!\" in next_word or \"?\" in next_word:\n",
    "                new_speech.append(next_word)\n",
    "                return(\" \".join(new_speech))\n",
    "            # If no punctuation append word to existing sentence\n",
    "            else:\n",
    "                new_speech.append(next_word)\n",
    "\n",
    "def hw2_part1():\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        Prints 5 sentences Trump could have said, based on trigram HMM.\n",
    "    \"\"\"\n",
    "    # Trump speeches file location\n",
    "    fname = \"speeches.txt\"\n",
    "    # Read the corpus\n",
    "    raw_corpus = codecs.open(fname,'r','utf8').read()\n",
    "    \n",
    "    # Tokenize the corpus\n",
    "    corpus = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "    # Generate list of trigrams\n",
    "    trump_trigr = list(nltk.trigrams(corpus))\n",
    "    \n",
    "    # Generate model\n",
    "    markov_stats = get_markov_stats(trump_trigr)\n",
    "   \n",
    "    # Generate and print sentences\n",
    "    for i in range(5):\n",
    "        sentence = generate_sentence(corpus, markov_stats)\n",
    "        print(sentence)    \n",
    "\n",
    "hw2_part1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Homework 2 part 2\n",
    "import pandas as pd \n",
    "\n",
    "# Function that splits a corpus in train and test\n",
    "def split_train_test(corpus,test_size=500):\n",
    "    return corpus[test_size:], corpus[:test_size]\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "\n",
    "def split_data(category, tag_set):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - category (string) category of the brown corpus\n",
    "        - tag_set (string) 'universal' or any other string that is not 'universal'\n",
    "    Output:\n",
    "        - train_tags (list of strings) training tags for default tagger\n",
    "        - train_tsents (corpus) tagged sentences training set\n",
    "        - test_tsents (corpus) tagged sentences test set\n",
    "    \n",
    "    \"\"\"\n",
    "    # Evaluate either universal or full tagset\n",
    "    if tag_set == 'universal':\n",
    "        brown_twords = brown.tagged_words(categories=category, tagset = 'universal')\n",
    "        brown_tsents = brown.tagged_sents(categories=category, tagset = 'universal')\n",
    "    else: \n",
    "        brown_twords = brown.tagged_words(categories=category)\n",
    "        brown_tsents = brown.tagged_sents(categories=category)\n",
    "\n",
    "    # Split each category in the brown corpus into train and test\n",
    "    train_twords, test_twords = split_train_test(brown_twords)\n",
    "    train_tsents, test_tsents = split_train_test(brown_tsents)\n",
    "    train_tags = [tag for (token,tag) in train_twords]\n",
    "    return(train_tags, train_tsents, test_tsents)\n",
    "\n",
    "def train_taggers(train_tags, train_tsents, test_tsents):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        - output of split_data()\n",
    "    Output:\n",
    "        - trained taggers\n",
    "    \"\"\"\n",
    "    \n",
    "    ### For each category, train and evaluate taggers. Use backoff.\n",
    "\n",
    "    # Default tagger (most frequent class)\n",
    "    start_time = time.time()\n",
    "    most_frequent_tag = nltk.FreqDist(train_tags).max()\n",
    "    default_tagger = nltk.DefaultTagger(most_frequent_tag)\n",
    "    default_time = time.time() - start_time\n",
    "    \n",
    "    # Affix tagger\n",
    "    start_time = time.time()\n",
    "    affix_tagger = AffixTagger(train_tsents)\n",
    "    affix_time = time.time() - start_time\n",
    "    \n",
    "    # Unigram tagger\n",
    "    start_time = time.time()\n",
    "    unigram_tagger = UnigramTagger(train_tsents, backoff = affix_tagger)\n",
    "    unigram_time = time.time() - start_time\n",
    "    \n",
    "    # Bigram tagger\n",
    "    start_time = time.time()\n",
    "    bigram_tagger = BigramTagger(train_tsents, backoff=unigram_tagger)\n",
    "    bigram_time = time.time() - start_time\n",
    "    \n",
    "    # Trigram tagger\n",
    "    start_time = time.time()\n",
    "    trigram_tagger = TrigramTagger(train_tsents, backoff=bigram_tagger)\n",
    "    trigram_time = time.time() - start_time\n",
    "    \n",
    "    taggers = [default_tagger, affix_tagger, unigram_tagger, bigram_tagger, trigram_tagger]\n",
    "    times = [default_time, affix_time, unigram_time, bigram_time, trigram_time]\n",
    "    return(taggers, times)\n",
    "\n",
    "def print_and_evaluate_accuracy(tag_set, taggers, category, test_set):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - tag_set (string) 'universal' or any other string that is not 'universal'\n",
    "        - taggers (list) trained taggers\n",
    "        - test_set (corpus) tsents output of split_data()\n",
    "    Outputs:\n",
    "        - taggerstr_list (list of strings) the names of the taggers\n",
    "        - accuracy_list (list) evaluated accuracies\n",
    "        - category_list (list of strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    tagger_strings = [\"default_tagger\", \"affix_tagger\", \"unigram_tagger\", \"bigram_tagger\", \"trigram_tagger\"]\n",
    "    taggerstr_list, accuracy_list, category_list = [], [], []\n",
    "    \n",
    "    # Print the statistics for all taggers provided the category\n",
    "    print(\"\\n\")\n",
    "    for tagger, tagstring in zip(taggers, tagger_strings):\n",
    "        accuracy = tagger.evaluate(test_set)\n",
    "        print(\"The accuracy of the {} tag set {} tagger on the {} category is: {}\".\\\n",
    "              format(tag_set, tagstring, category, round(accuracy,2)))\n",
    "\n",
    "        taggerstr_list.append(tagstring)\n",
    "        accuracy_list.append(accuracy)\n",
    "        category_list.append(category)\n",
    "    return(taggerstr_list, accuracy_list, category_list)\n",
    "    \n",
    "\n",
    "def get_tagger_summary(tag_set, start_cat, end_cat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - start_cat (integers) category in the brown corpus to start with\n",
    "        - end_cat --//-- end with\n",
    "    \"\"\"\n",
    "    test_sents_list, taggers_list, taggerstr_list, accuracy_list, category_list,times_list = [], [], [], [], [], []\n",
    "    \n",
    "    for category in brown.categories()[start_cat:end_cat]:\n",
    "        train_tags, train_tsents, test_tsents = split_data(category, tag_set)\n",
    "        taggers,times = train_taggers(train_tags, train_tsents, test_tsents)\n",
    "        taggerstr_, accuracy_, category_ = print_and_evaluate_accuracy(tag_set, taggers, category, test_tsents)\n",
    "        taggerstr_list.extend(taggerstr_)\n",
    "        accuracy_list.extend(accuracy_)\n",
    "        category_list.extend(category_) \n",
    "        taggers_list.extend(taggers)\n",
    "        times_list.extend(times)\n",
    "        test_sents_list.extend(test_tsents)\n",
    "        \n",
    "    summary = pd.DataFrame({\n",
    "        \"tagger\": taggerstr_list,\n",
    "        \"accuracy\": accuracy_list,\n",
    "        \"category\": category_list,\n",
    "        \"runtime\" : times_list\n",
    "                      })\n",
    "    cross_evaluation = pd.DataFrame({\n",
    "        \"category\": category_list,\n",
    "        \"tagger\": taggers_list\n",
    "    })\n",
    "    return(summary, cross_evaluation)\n",
    " \n",
    "def print_best_tagger(df):\n",
    "    \"\"\"\n",
    "    Input:  \n",
    "        - df: first output of get_tagger_summary()\n",
    "    \"\"\"\n",
    "    ### Prints which is the best performing tagger for each category in df\n",
    "    idx = df.groupby(['category'])['accuracy'].transform(max) == df['accuracy']\n",
    "    max_ = df[idx]\n",
    "    print(\"\\nThe best performing tagger for each category is given by\\n\", max_)\n",
    "    \n",
    "def train_and_evaluate_nb_tagger(tag_set, category = \"news\"):\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (full tagset)\n",
    "    if tag_set == \"universal\":\n",
    "        brown_tsents = brown.tagged_sents(categories=category, tagset = \"universal\")\n",
    "    else:\n",
    "        brown_tsents = brown.tagged_sents(categories=category)\n",
    "    train_tsents, test_tsents = split_train_test(brown_tsents)\n",
    "    start_time = time.time()\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_tsents)\n",
    "    nb_time = time.time() - start_time\n",
    "    accuracy = nb_tagger.evaluate(test_tsents)\n",
    "\n",
    "    # Print the performance of the nb tagger and the runtime (full tagset)\n",
    "    print(\"\\nThe accuracy of the nb tagger using the {} tagset on the news category is: {}\".\\\n",
    "          format(tag_set, round(accuracy,2)))\n",
    "    print(\"\\nThe runtime of the nb tagger using the {} tagset on the news category in seconds is: {}\".\\\n",
    "          format(tag_set, round(nb_time,2)))\n",
    "    \n",
    "def evaluate_cross_accuracy(model, train_category, test_category, tag_set = 'universal'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - model (df) trained taggers, second output of get_tagger_summary()\n",
    "    Outputs:\n",
    "        - summary_cross (df) summarizes all the results of cross evaluation in a df \n",
    "    \n",
    "    \"\"\"\n",
    "    _, _, test_sents = split_data(test_category, tag_set)\n",
    "    taggers = model[model[\"category\"] == train_category][\"tagger\"]\n",
    "    accuracies = [round(tagger.evaluate(test_sents),2) for tagger in taggers]        \n",
    "    train_cat_list = taggers.shape[0] * [train_category]\n",
    "    test_cat_list = taggers.shape[0] * [test_category]\n",
    "    \n",
    "    summary_cross = pd.DataFrame({\n",
    "        'tagger': taggers,\n",
    "        'accuracy': accuracies,\n",
    "        'train_category': train_cat_list,\n",
    "        'test_category': test_cat_list\n",
    "    })\n",
    "    return(summary_cross)\n",
    "    \n",
    "def hw2_part2(start, stop):\n",
    "    \n",
    "    ### FULL TAGSET\n",
    "    ftag, cross_eval_ftag = get_tagger_summary('full', start, stop)\n",
    "        \n",
    "    ### UNIVERSAL TAGSET\n",
    "    utag, cross_eval_utag = get_tagger_summary('universal', start, stop)\n",
    "       \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (full tagset)\n",
    "    print_best_tagger(ftag)\n",
    "    \n",
    "    # NB classifier on full tagset\n",
    "    train_and_evaluate_nb_tagger(\"full\")\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (universal tagset)\n",
    "    print_best_tagger(utag)\n",
    "    \n",
    "    # NB classifier on universal tagset\n",
    "    train_and_evaluate_nb_tagger(\"universal\")\n",
    "    \n",
    "    ### Cross evaluation\n",
    "\n",
    "    # Cross-evaluate between categories (using universal tagset)\n",
    "    df_cross_eval = pd.DataFrame(columns = ['train_category', 'test_category', 'tagger', 'accuracy'])\n",
    "    \n",
    "    for train_category in brown.categories()[start:stop]:\n",
    "        categories = brown.categories()[start:stop]\n",
    "        categories.remove(train_category)\n",
    "        for test_category in categories:\n",
    "            df_cross_eval = pd.concat([df_cross_eval, \\\n",
    "                                       evaluate_cross_accuracy(cross_eval_utag, train_category, test_category)])\n",
    "    \n",
    "    # Example: train on news_train, evaluate on the \"test\" of every other category\n",
    "    # Do this for all categories in the corpus\n",
    "    # Print the results\n",
    "    print(\"\\nThe cross evaluation gives the following result:\")\n",
    "    print(df_cross_eval)\n",
    "    return(df_cross_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'The', u'AT'), (u'Fulton', u'NP-TL'), (u'County', u'NN-TL'), (u'Grand', u'JJ-TL'), (u'Jury', u'NN-TL'), (u'said', u'VBD'), (u'Friday', u'NR'), (u'an', u'AT'), (u'investigation', u'NN'), (u'of', u'IN'), (u\"Atlanta's\", u'NP$'), (u'recent', u'JJ'), (u'primary', u'NN'), (u'election', u'NN'), (u'produced', u'VBD'), (u'``', u'``'), (u'no', u'AT'), (u'evidence', u'NN'), (u\"''\", u\"''\"), (u'that', u'CS'), (u'any', u'DTI'), (u'irregularities', u'NNS'), (u'took', u'VBD'), (u'place', u'NN'), (u'.', u'.')], [(u'The', u'AT'), (u'jury', u'NN'), (u'further', u'RBR'), (u'said', u'VBD'), (u'in', u'IN'), (u'term-end', u'NN'), (u'presentments', u'NNS'), (u'that', u'CS'), (u'the', u'AT'), (u'City', u'NN-TL'), (u'Executive', u'JJ-TL'), (u'Committee', u'NN-TL'), (u',', u','), (u'which', u'WDT'), (u'had', u'HVD'), (u'over-all', u'JJ'), (u'charge', u'NN'), (u'of', u'IN'), (u'the', u'AT'), (u'election', u'NN'), (u',', u','), (u'``', u'``'), (u'deserves', u'VBZ'), (u'the', u'AT'), (u'praise', u'NN'), (u'and', u'CC'), (u'thanks', u'NNS'), (u'of', u'IN'), (u'the', u'AT'), (u'City', u'NN-TL'), (u'of', u'IN-TL'), (u'Atlanta', u'NP-TL'), (u\"''\", u\"''\"), (u'for', u'IN'), (u'the', u'AT'), (u'manner', u'NN'), (u'in', u'IN'), (u'which', u'WDT'), (u'the', u'AT'), (u'election', u'NN'), (u'was', u'BEDZ'), (u'conducted', u'VBN'), (u'.', u'.')], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags, train_tsents, test_tsents = split_data('news', 'unviersal')\n",
    "test_tsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the adventure category is: 0.11\n",
      "The accuracy of the full tag set affix_tagger tagger on the adventure category is: 0.19\n",
      "The accuracy of the full tag set unigram_tagger tagger on the adventure category is: 0.89\n",
      "The accuracy of the full tag set bigram_tagger tagger on the adventure category is: 0.91\n",
      "The accuracy of the full tag set trigram_tagger tagger on the adventure category is: 0.9\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the belles_lettres category is: 0.12\n",
      "The accuracy of the full tag set affix_tagger tagger on the belles_lettres category is: 0.26\n",
      "The accuracy of the full tag set unigram_tagger tagger on the belles_lettres category is: 0.9\n",
      "The accuracy of the full tag set bigram_tagger tagger on the belles_lettres category is: 0.91\n",
      "The accuracy of the full tag set trigram_tagger tagger on the belles_lettres category is: 0.91\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the editorial category is: 0.14\n",
      "The accuracy of the full tag set affix_tagger tagger on the editorial category is: 0.26\n",
      "The accuracy of the full tag set unigram_tagger tagger on the editorial category is: 0.86\n",
      "The accuracy of the full tag set bigram_tagger tagger on the editorial category is: 0.87\n",
      "The accuracy of the full tag set trigram_tagger tagger on the editorial category is: 0.87\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the fiction category is: 0.12\n",
      "The accuracy of the full tag set affix_tagger tagger on the fiction category is: 0.2\n",
      "The accuracy of the full tag set unigram_tagger tagger on the fiction category is: 0.87\n",
      "The accuracy of the full tag set bigram_tagger tagger on the fiction category is: 0.88\n",
      "The accuracy of the full tag set trigram_tagger tagger on the fiction category is: 0.88\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the government category is: 0.14\n",
      "The accuracy of the full tag set affix_tagger tagger on the government category is: 0.29\n",
      "The accuracy of the full tag set unigram_tagger tagger on the government category is: 0.84\n",
      "The accuracy of the full tag set bigram_tagger tagger on the government category is: 0.86\n",
      "The accuracy of the full tag set trigram_tagger tagger on the government category is: 0.86\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the hobbies category is: 0.13\n",
      "The accuracy of the full tag set affix_tagger tagger on the hobbies category is: 0.23\n",
      "The accuracy of the full tag set unigram_tagger tagger on the hobbies category is: 0.84\n",
      "The accuracy of the full tag set bigram_tagger tagger on the hobbies category is: 0.85\n",
      "The accuracy of the full tag set trigram_tagger tagger on the hobbies category is: 0.85\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the humor category is: 0.12\n",
      "The accuracy of the full tag set affix_tagger tagger on the humor category is: 0.19\n",
      "The accuracy of the full tag set unigram_tagger tagger on the humor category is: 0.8\n",
      "The accuracy of the full tag set bigram_tagger tagger on the humor category is: 0.8\n",
      "The accuracy of the full tag set trigram_tagger tagger on the humor category is: 0.8\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the learned category is: 0.2\n",
      "The accuracy of the full tag set affix_tagger tagger on the learned category is: 0.32\n",
      "The accuracy of the full tag set unigram_tagger tagger on the learned category is: 0.91\n",
      "The accuracy of the full tag set bigram_tagger tagger on the learned category is: 0.91\n",
      "The accuracy of the full tag set trigram_tagger tagger on the learned category is: 0.91\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the lore category is: 0.14\n",
      "The accuracy of the full tag set affix_tagger tagger on the lore category is: 0.24\n",
      "The accuracy of the full tag set unigram_tagger tagger on the lore category is: 0.85\n",
      "The accuracy of the full tag set bigram_tagger tagger on the lore category is: 0.86\n",
      "The accuracy of the full tag set trigram_tagger tagger on the lore category is: 0.86\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the mystery category is: 0.11\n",
      "The accuracy of the full tag set affix_tagger tagger on the mystery category is: 0.18\n",
      "The accuracy of the full tag set unigram_tagger tagger on the mystery category is: 0.87\n",
      "The accuracy of the full tag set bigram_tagger tagger on the mystery category is: 0.89\n",
      "The accuracy of the full tag set trigram_tagger tagger on the mystery category is: 0.89\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the news category is: 0.14\n",
      "The accuracy of the full tag set affix_tagger tagger on the news category is: 0.26\n",
      "The accuracy of the full tag set unigram_tagger tagger on the news category is: 0.88\n",
      "The accuracy of the full tag set bigram_tagger tagger on the news category is: 0.89\n",
      "The accuracy of the full tag set trigram_tagger tagger on the news category is: 0.89\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the religion category is: 0.12\n",
      "The accuracy of the full tag set affix_tagger tagger on the religion category is: 0.23\n",
      "The accuracy of the full tag set unigram_tagger tagger on the religion category is: 0.86\n",
      "The accuracy of the full tag set bigram_tagger tagger on the religion category is: 0.87\n",
      "The accuracy of the full tag set trigram_tagger tagger on the religion category is: 0.86\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the reviews category is: 0.12\n",
      "The accuracy of the full tag set affix_tagger tagger on the reviews category is: 0.23\n",
      "The accuracy of the full tag set unigram_tagger tagger on the reviews category is: 0.84\n",
      "The accuracy of the full tag set bigram_tagger tagger on the reviews category is: 0.84\n",
      "The accuracy of the full tag set trigram_tagger tagger on the reviews category is: 0.84\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the romance category is: 0.11\n",
      "The accuracy of the full tag set affix_tagger tagger on the romance category is: 0.2\n",
      "The accuracy of the full tag set unigram_tagger tagger on the romance category is: 0.86\n",
      "The accuracy of the full tag set bigram_tagger tagger on the romance category is: 0.87\n",
      "The accuracy of the full tag set trigram_tagger tagger on the romance category is: 0.87\n",
      "\n",
      "\n",
      "The accuracy of the full tag set default_tagger tagger on the science_fiction category is: 0.1\n",
      "The accuracy of the full tag set affix_tagger tagger on the science_fiction category is: 0.16\n",
      "The accuracy of the full tag set unigram_tagger tagger on the science_fiction category is: 0.77\n",
      "The accuracy of the full tag set bigram_tagger tagger on the science_fiction category is: 0.77\n",
      "The accuracy of the full tag set trigram_tagger tagger on the science_fiction category is: 0.77\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the adventure category is: 0.18\n",
      "The accuracy of the universal tag set affix_tagger tagger on the adventure category is: 0.22\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the adventure category is: 0.92\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the adventure category is: 0.93\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the adventure category is: 0.93\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the belles_lettres category is: 0.22\n",
      "The accuracy of the universal tag set affix_tagger tagger on the belles_lettres category is: 0.3\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the belles_lettres category is: 0.93\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the belles_lettres category is: 0.94\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the belles_lettres category is: 0.93\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the editorial category is: 0.27\n",
      "The accuracy of the universal tag set affix_tagger tagger on the editorial category is: 0.32\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the editorial category is: 0.91\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the editorial category is: 0.92\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the editorial category is: 0.92\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the fiction category is: 0.2\n",
      "The accuracy of the universal tag set affix_tagger tagger on the fiction category is: 0.25\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the fiction category is: 0.92\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the fiction category is: 0.92\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the fiction category is: 0.92\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the government category is: 0.28\n",
      "The accuracy of the universal tag set affix_tagger tagger on the government category is: 0.35\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the government category is: 0.91\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the government category is: 0.92\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the government category is: 0.92\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the hobbies category is: 0.23\n",
      "The accuracy of the universal tag set affix_tagger tagger on the hobbies category is: 0.28\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the hobbies category is: 0.89\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the hobbies category is: 0.89\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the hobbies category is: 0.89\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the humor category is: 0.21\n",
      "The accuracy of the universal tag set affix_tagger tagger on the humor category is: 0.23\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the humor category is: 0.85\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the humor category is: 0.85\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the humor category is: 0.85\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the learned category is: 0.28\n",
      "The accuracy of the universal tag set affix_tagger tagger on the learned category is: 0.35\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the learned category is: 0.94\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the learned category is: 0.94\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the learned category is: 0.94\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the lore category is: 0.25\n",
      "The accuracy of the universal tag set affix_tagger tagger on the lore category is: 0.29\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the lore category is: 0.9\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the lore category is: 0.91\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the lore category is: 0.91\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the mystery category is: 0.17\n",
      "The accuracy of the universal tag set affix_tagger tagger on the mystery category is: 0.21\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the mystery category is: 0.91\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the mystery category is: 0.91\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the mystery category is: 0.91\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the news category is: 0.3\n",
      "The accuracy of the universal tag set affix_tagger tagger on the news category is: 0.35\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the news category is: 0.93\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the news category is: 0.94\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the news category is: 0.94\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the religion category is: 0.21\n",
      "The accuracy of the universal tag set affix_tagger tagger on the religion category is: 0.28\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the religion category is: 0.9\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the religion category is: 0.9\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the religion category is: 0.9\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the reviews category is: 0.25\n",
      "The accuracy of the universal tag set affix_tagger tagger on the reviews category is: 0.29\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the reviews category is: 0.9\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the reviews category is: 0.9\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the reviews category is: 0.9\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the romance category is: 0.18\n",
      "The accuracy of the universal tag set affix_tagger tagger on the romance category is: 0.24\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the romance category is: 0.9\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the romance category is: 0.9\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the romance category is: 0.9\n",
      "\n",
      "\n",
      "The accuracy of the universal tag set default_tagger tagger on the science_fiction category is: 0.18\n",
      "The accuracy of the universal tag set affix_tagger tagger on the science_fiction category is: 0.2\n",
      "The accuracy of the universal tag set unigram_tagger tagger on the science_fiction category is: 0.82\n",
      "The accuracy of the universal tag set bigram_tagger tagger on the science_fiction category is: 0.82\n",
      "The accuracy of the universal tag set trigram_tagger tagger on the science_fiction category is: 0.82\n",
      "\n",
      "The best performing tagger for each category is given by\n",
      "     accuracy         category   runtime          tagger\n",
      "3   0.905660        adventure  0.492621   bigram_tagger\n",
      "9   0.909212   belles_lettres  1.630930  trigram_tagger\n",
      "13  0.868889        editorial  0.480835   bigram_tagger\n",
      "18  0.882862          fiction  0.541420   bigram_tagger\n",
      "19  0.882862          fiction  0.663931  trigram_tagger\n",
      "24  0.857568       government  0.639616  trigram_tagger\n",
      "29  0.847869          hobbies  0.759401  trigram_tagger\n",
      "34  0.802843            humor  0.107629  trigram_tagger\n",
      "39  0.913854          learned  1.754479  trigram_tagger\n",
      "43  0.864708             lore  0.832561   bigram_tagger\n",
      "48  0.889661          mystery  0.434439   bigram_tagger\n",
      "53  0.888225             news  0.792946   bigram_tagger\n",
      "58  0.865193         religion  0.254862   bigram_tagger\n",
      "64  0.844573          reviews  0.282999  trigram_tagger\n",
      "68  0.867322          romance  0.504995   bigram_tagger\n",
      "73  0.774850  science_fiction  0.102353   bigram_tagger\n",
      "\n",
      "The accuracy of the nb tagger using the full tagset on the news category is: 0.91\n",
      "\n",
      "The runtime of the nb tagger using the full tagset on the news category in seconds is: 2.9\n",
      "\n",
      "The best performing tagger for each category is given by\n",
      "     accuracy         category   runtime          tagger\n",
      "3   0.933962        adventure  0.611088   bigram_tagger\n",
      "8   0.936496   belles_lettres  1.370187   bigram_tagger\n",
      "13  0.917222        editorial  0.468174   bigram_tagger\n",
      "19  0.919684          fiction  0.617273  trigram_tagger\n",
      "24  0.917237       government  0.590530  trigram_tagger\n",
      "28  0.888784          hobbies  0.682070   bigram_tagger\n",
      "32  0.853944            humor  0.085556  unigram_tagger\n",
      "38  0.941613          learned  1.478530   bigram_tagger\n",
      "43  0.908563             lore  0.846005   bigram_tagger\n",
      "48  0.914998          mystery  0.458259   bigram_tagger\n",
      "53  0.938092             news  0.880731   bigram_tagger\n",
      "58  0.901720         religion  0.223649   bigram_tagger\n",
      "63  0.898705          reviews  0.275710   bigram_tagger\n",
      "68  0.900865          romance  0.558321   bigram_tagger\n",
      "72  0.821941  science_fiction  0.058825  unigram_tagger\n",
      "\n",
      "The accuracy of the nb tagger using the universal tagset on the news category is: 0.93\n",
      "\n",
      "The runtime of the nb tagger using the universal tagset on the news category in seconds is: 2.8\n",
      "\n",
      "The cross evaluation gives the following result:\n",
      "    accuracy                      tagger   test_category   train_category\n",
      "0       0.22   <DefaultTagger: tag=NOUN>  belles_lettres        adventure\n",
      "1       0.28     <AffixTagger: size=996>  belles_lettres        adventure\n",
      "2       0.89  <UnigramTagger: size=3013>  belles_lettres        adventure\n",
      "3       0.90    <BigramTagger: size=779>  belles_lettres        adventure\n",
      "4       0.90   <TrigramTagger: size=666>  belles_lettres        adventure\n",
      "0       0.27   <DefaultTagger: tag=NOUN>       editorial        adventure\n",
      "1       0.29     <AffixTagger: size=996>       editorial        adventure\n",
      "2       0.86  <UnigramTagger: size=3013>       editorial        adventure\n",
      "3       0.87    <BigramTagger: size=779>       editorial        adventure\n",
      "4       0.86   <TrigramTagger: size=666>       editorial        adventure\n",
      "0       0.20   <DefaultTagger: tag=NOUN>         fiction        adventure\n",
      "1       0.24     <AffixTagger: size=996>         fiction        adventure\n",
      "2       0.91  <UnigramTagger: size=3013>         fiction        adventure\n",
      "3       0.92    <BigramTagger: size=779>         fiction        adventure\n",
      "4       0.92   <TrigramTagger: size=666>         fiction        adventure\n",
      "0       0.28   <DefaultTagger: tag=NOUN>      government        adventure\n",
      "1       0.32     <AffixTagger: size=996>      government        adventure\n",
      "2       0.87  <UnigramTagger: size=3013>      government        adventure\n",
      "3       0.87    <BigramTagger: size=779>      government        adventure\n",
      "4       0.87   <TrigramTagger: size=666>      government        adventure\n",
      "0       0.23   <DefaultTagger: tag=NOUN>         hobbies        adventure\n",
      "1       0.27     <AffixTagger: size=996>         hobbies        adventure\n",
      "2       0.87  <UnigramTagger: size=3013>         hobbies        adventure\n",
      "3       0.87    <BigramTagger: size=779>         hobbies        adventure\n",
      "4       0.87   <TrigramTagger: size=666>         hobbies        adventure\n",
      "0       0.21   <DefaultTagger: tag=NOUN>           humor        adventure\n",
      "1       0.25     <AffixTagger: size=996>           humor        adventure\n",
      "2       0.90  <UnigramTagger: size=3013>           humor        adventure\n",
      "3       0.90    <BigramTagger: size=779>           humor        adventure\n",
      "4       0.90   <TrigramTagger: size=666>           humor        adventure\n",
      "..       ...                         ...             ...              ...\n",
      "70      0.25   <DefaultTagger: tag=NOUN>            lore  science_fiction\n",
      "71      0.24     <AffixTagger: size=443>            lore  science_fiction\n",
      "72      0.78   <UnigramTagger: size=705>            lore  science_fiction\n",
      "73      0.78     <BigramTagger: size=92>            lore  science_fiction\n",
      "74      0.78    <TrigramTagger: size=89>            lore  science_fiction\n",
      "70      0.17   <DefaultTagger: tag=NOUN>         mystery  science_fiction\n",
      "71      0.19     <AffixTagger: size=443>         mystery  science_fiction\n",
      "72      0.83   <UnigramTagger: size=705>         mystery  science_fiction\n",
      "73      0.83     <BigramTagger: size=92>         mystery  science_fiction\n",
      "74      0.82    <TrigramTagger: size=89>         mystery  science_fiction\n",
      "70      0.30   <DefaultTagger: tag=NOUN>            news  science_fiction\n",
      "71      0.27     <AffixTagger: size=443>            news  science_fiction\n",
      "72      0.76   <UnigramTagger: size=705>            news  science_fiction\n",
      "73      0.75     <BigramTagger: size=92>            news  science_fiction\n",
      "74      0.75    <TrigramTagger: size=89>            news  science_fiction\n",
      "70      0.21   <DefaultTagger: tag=NOUN>        religion  science_fiction\n",
      "71      0.24     <AffixTagger: size=443>        religion  science_fiction\n",
      "72      0.81   <UnigramTagger: size=705>        religion  science_fiction\n",
      "73      0.81     <BigramTagger: size=92>        religion  science_fiction\n",
      "74      0.81    <TrigramTagger: size=89>        religion  science_fiction\n",
      "70      0.25   <DefaultTagger: tag=NOUN>         reviews  science_fiction\n",
      "71      0.23     <AffixTagger: size=443>         reviews  science_fiction\n",
      "72      0.77   <UnigramTagger: size=705>         reviews  science_fiction\n",
      "73      0.77     <BigramTagger: size=92>         reviews  science_fiction\n",
      "74      0.77    <TrigramTagger: size=89>         reviews  science_fiction\n",
      "70      0.19   <DefaultTagger: tag=NOUN>         romance  science_fiction\n",
      "71      0.20     <AffixTagger: size=443>         romance  science_fiction\n",
      "72      0.80   <UnigramTagger: size=705>         romance  science_fiction\n",
      "73      0.80     <BigramTagger: size=92>         romance  science_fiction\n",
      "74      0.80    <TrigramTagger: size=89>         romance  science_fiction\n",
      "\n",
      "[1050 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "summary = hw2_part2(0, len(brown.categories()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tagger</th>\n",
       "      <th>test_category</th>\n",
       "      <th>train_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.22</td>\n",
       "      <td>&lt;DefaultTagger: tag=NOUN&gt;</td>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;AffixTagger: size=996&gt;</td>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>&lt;UnigramTagger: size=3013&gt;</td>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>&lt;BigramTagger: size=779&gt;</td>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>&lt;TrigramTagger: size=666&gt;</td>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy                      tagger   test_category train_category\n",
       "0      0.22   <DefaultTagger: tag=NOUN>  belles_lettres      adventure\n",
       "1      0.28     <AffixTagger: size=996>  belles_lettres      adventure\n",
       "2      0.89  <UnigramTagger: size=3013>  belles_lettres      adventure\n",
       "3      0.90    <BigramTagger: size=779>  belles_lettres      adventure\n",
       "4      0.90   <TrigramTagger: size=666>  belles_lettres      adventure"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe containing the cross validation\n",
    "summary.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
